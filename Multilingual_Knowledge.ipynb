{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b69198cc"
   },
   "source": [
    "### Install Dependencies\n",
    "\n",
    "This cell installs all the necessary Python libraries like `pdf2image`, `pillow`, `requests`, `sentence-transformers`, `faiss-cpu`, `langchain`, `langchain-community`, and `langchain-text-splitters`. It also installs `poppler-utils`, a command-line utility for working with PDFs, which `pdf2image` relies on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b9f2608",
    "outputId": "db27ccdc-ba93-470f-eb82-a9ac431a5791"
   },
   "outputs": [],
   "source": [
    "!pip install -q pdf2image pillow requests sentence-transformers faiss-cpu langchain langchain-community langchain-text-splitters\n",
    "!apt-get install -y poppler-utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2baf2a72"
   },
   "source": [
    "### Import Libraries\n",
    "\n",
    "This cell imports all the required modules for the project, including `os`, `re`, `json`, `time`, `base64`, `requests`, `numpy`, `faiss`, `pdf2image`, `google.colab.userdata`, `langchain_text_splitters`, and `sentence_transformers`. These libraries are used for file operations, regular expressions, JSON handling, time delays, base64 encoding, making HTTP requests, numerical operations, vector indexing, PDF to image conversion, accessing Colab secrets, text splitting, and embedding generation, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f204ea1",
    "outputId": "31039974-24a6-4b5d-e0f3-1025ccf99c26"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import base64\n",
    "import requests\n",
    "import numpy as np\n",
    "import faiss\n",
    "from pdf2image import convert_from_path\n",
    "from google.colab import userdata\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ae94a58b"
   },
   "source": [
    "### Configuration Class\n",
    "\n",
    "This `Config` class defines all the essential parameters for the application, such as the `OPENROUTER_API_KEY` (retrieved from Colab secrets), the `MODEL_NAME` for the LLM, the `EMBEDDING_MODEL` for generating text embeddings, the `PDF_PATH` of the input document, and the `OUTPUT_JSON` file path for storing processed data. It also includes a check to ensure the API key is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2a7d8477"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    API_KEY = userdata.get(\"OPENROUTER_API_KEY\") # Ensure this is set in Colab Secrets\n",
    "    MODEL_NAME = \"google/gemini-2.0-flash-001\" # Faster & cheaper than 1.5 Pro\n",
    "    EMBEDDING_MODEL = \"intfloat/multilingual-e5-large\"\n",
    "    PDF_PATH = \"/content/test.pdf\" # Upload your file here\n",
    "    OUTPUT_JSON = \"/content/structured_knowledge.json\"\n",
    "\n",
    "    if not API_KEY:\n",
    "        raise ValueError(\" OPENROUTER_API_KEY not found in Secrets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a54b22dd"
   },
   "source": [
    "### Document Processor Class\n",
    "\n",
    "The `DocumentProcessor` class is responsible for extracting text content from PDF pages. It uses `pdf2image` to convert PDF pages into images and then sends these images to an LLM (via OpenRouter API) for layout-aware OCR. The `_encode_image` method handles base64 encoding of images, and `extract_page_content` constructs a prompt to instruct the LLM to output structured Markdown. The `process_pdf` method iterates through all pages, extracts content, and saves it as a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02a28156"
   },
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {config.API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"HTTP-Referer\": \"https://colab.research.google.com\"\n",
    "        }\n",
    "\n",
    "    def _encode_image(self, image_path):\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "    def extract_page_content(self, image_path, page_num):\n",
    "        \"\"\"Sends page image to LLM for Layout-Aware OCR\"\"\"\n",
    "        b64_image = self._encode_image(image_path)\n",
    "\n",
    "        # PROMPT ENGINEERING: Enforce Markdown structure for downstream chunking\n",
    "        prompt = \"\"\"\n",
    "        Analyze this textbook page (Gujarati/Sanskrit). extract the text while strictly preserving structure using Markdown:\n",
    "        1. Use '#' for Main Titles (like Book Name).\n",
    "        2. Use '##' for Chapters (Adhyay).\n",
    "        3. Use '###' for Verses (Shlokas) or Sub-sections.\n",
    "        4. Keep Sanskrit Shlokas and Gujarati commentary distinct.\n",
    "        5. Do NOT translate. Output raw text exactly as seen.\n",
    "        \"\"\"\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.config.MODEL_NAME,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{b64_image}\"}}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=self.headers, json=payload)\n",
    "            response.raise_for_status()\n",
    "            return response.json()['choices'][0]['message']['content']\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error on page {page_num}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def process_pdf(self):\n",
    "        print(f\"üöÄ Processing PDF: {self.config.PDF_PATH}\")\n",
    "        images = convert_from_path(self.config.PDF_PATH, dpi=150) # 150 DPI is enough for LLMs\n",
    "        full_document = []\n",
    "\n",
    "        for i, img in enumerate(images):\n",
    "            page_num = i + 1\n",
    "            print(f\"   üìÑ Scanning Page {page_num}/{len(images)}...\")\n",
    "\n",
    "            temp_path = f\"temp_page_{page_num}.jpg\"\n",
    "            img.save(temp_path, \"JPEG\")\n",
    "\n",
    "            content = self.extract_page_content(temp_path, page_num)\n",
    "\n",
    "            # Structuring the raw data\n",
    "            full_document.append({\n",
    "                \"page\": page_num,\n",
    "                \"content\": content\n",
    "            })\n",
    "\n",
    "            os.remove(temp_path)\n",
    "            time.sleep(1) # Rate limit politeness\n",
    "\n",
    "        # Save as JSON (Better than Docx for data)\n",
    "        with open(self.config.OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(full_document, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"‚úÖ OCR Complete. Saved to {self.config.OUTPUT_JSON}\")\n",
    "        return full_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "154506de"
   },
   "source": [
    "### Knowledge Base Class\n",
    "\n",
    "The `KnowledgeBase` class manages the creation and retrieval of information from the processed PDF. The `chunk_data` method uses `MarkdownHeaderTextSplitter` and `RecursiveCharacterTextSplitter` from `langchain_text_splitters` to divide the document into smaller, context-rich chunks while preserving hierarchical information (like Book Title, Chapter, Section). The `build_index` method generates embeddings for these chunks using a `SentenceTransformer` model and stores them in a FAISS vector index. The `retrieve` method then uses this index to find the most relevant chunks given a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7493b5c"
   },
   "outputs": [],
   "source": [
    "class KnowledgeBase:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.embedder = SentenceTransformer(config.EMBEDDING_MODEL)\n",
    "        self.index = None\n",
    "        self.chunks = []\n",
    "\n",
    "    def chunk_data(self, json_data):\n",
    "        print(\"üß© Chunking data with Hierarchy Preservation...\")\n",
    "\n",
    "        # 1. Split by Headers first (Keep Chapter context)\n",
    "        headers_to_split_on = [\n",
    "            (\"#\", \"Book Title\"),\n",
    "            (\"##\", \"Chapter\"),\n",
    "            (\"###\", \"Section\"),\n",
    "        ]\n",
    "        markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "        # 2. Recursive split for long sections\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "        processed_chunks = []\n",
    "\n",
    "        for page in json_data:\n",
    "            # First, split by Markdown headers\n",
    "            md_docs = markdown_splitter.split_text(page['content'])\n",
    "\n",
    "            for doc in md_docs:\n",
    "                # Further split long sections if needed\n",
    "                splits = text_splitter.split_text(doc.page_content)\n",
    "\n",
    "                for split in splits:\n",
    "                    processed_chunks.append({\n",
    "                        \"text\": split,\n",
    "                        \"metadata\": {\n",
    "                            \"page\": page['page'],\n",
    "                            **doc.metadata # Inherits \"Chapter\", \"Book Title\" from headers\n",
    "                        }\n",
    "                    })\n",
    "\n",
    "        self.chunks = processed_chunks\n",
    "        print(f\"‚úÖ Created {len(self.chunks)} knowledge chunks.\")\n",
    "        return processed_chunks\n",
    "\n",
    "    def build_index(self):\n",
    "        print(\"üß† Generating Embeddings (This may take a moment)...\")\n",
    "        texts = [c[\"text\"] for c in self.chunks]\n",
    "\n",
    "        # E5 requires \"passage: \" prefix for documents\n",
    "        texts_for_embedding = [f\"passage: {t}\" for t in texts]\n",
    "\n",
    "        embeddings = self.embedder.encode(texts_for_embedding, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(dimension)\n",
    "        self.index.add(embeddings)\n",
    "        print(\"‚úÖ Vector Index Built.\")\n",
    "\n",
    "    def retrieve(self, query, k=4):\n",
    "        # E5 requires \"query: \" prefix for queries\n",
    "        query_vec = self.embedder.encode([f\"query: {query}\"], convert_to_numpy=True, normalize_embeddings=True)\n",
    "        distances, indices = self.index.search(query_vec, k)\n",
    "\n",
    "        results = []\n",
    "        for idx in indices[0]:\n",
    "            results.append(self.chunks[idx])\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "149a6883"
   },
   "source": [
    "### RAG Assistant Class\n",
    "\n",
    "The `RAGAssistant` class integrates the retrieval and generation components. When asked a question, it uses the `KnowledgeBase` to `retrieve` relevant document chunks. It then formats these retrieved documents as `context` and constructs a prompt for the LLM. The prompt instructs the LLM to answer the question *only* based on the provided context, enabling a Retrieval-Augmented Generation (RAG) approach to ensure factual and grounded responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "db48d0a0"
   },
   "outputs": [],
   "source": [
    "class RAGAssistant:\n",
    "    def __init__(self, config, kb):\n",
    "        self.config = config\n",
    "        self.kb = kb\n",
    "\n",
    "    def ask(self, query):\n",
    "        retrieved_docs = self.kb.retrieve(query)\n",
    "\n",
    "        # Format context with Metadata for the LLM\n",
    "        context_str = \"\"\n",
    "        for i, doc in enumerate(retrieved_docs):\n",
    "            meta = doc['metadata']\n",
    "            chapter_info = f\" [Chapter: {meta.get('Chapter', 'General')}]\" if 'Chapter' in meta else \"\"\n",
    "            context_str += f\"Source {i+1} (Page {meta['page']}{chapter_info}):\\n{doc['text']}\\n\\n\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are an expert on the Bhagavad Gita. Answer the question based ONLY on the context below.\n",
    "\n",
    "        CONTEXT:\n",
    "        {context_str}\n",
    "\n",
    "        QUESTION: {query}\n",
    "\n",
    "        ANSWER (in the same language as the question):\n",
    "        \"\"\"\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.config.MODEL_NAME,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": 0.3\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "                                     headers={\"Authorization\": f\"Bearer {self.config.API_KEY}\"},\n",
    "                                     json=payload)\n",
    "            return response.json()['choices'][0]['message']['content']\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "585a618a"
   },
   "source": [
    "### Main Execution Block\n",
    "\n",
    "This is the main entry point of the script. It initializes the `DocumentProcessor` and checks if the OCR data (`structured_knowledge.json`) already exists. If not, it processes the PDF to extract content. Then, it initializes the `KnowledgeBase`, chunks the data, and builds the FAISS index. Finally, it sets up the `RAGAssistant` and enters an interactive loop, allowing the user to ask questions about the PDF content until they type 'exit' or 'quit'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "5cfc801149da4421bd7b1d1f23bc30bf",
      "755c0570b9c7461fa908a4d42827109a",
      "2d50f087d9b34d40aa3fd3c3b68636dc",
      "e708c0a60c204dc5af89dcfbbff52123",
      "d25d4455edf340b7b8c1de5888d6442c",
      "499ce69658fb44ee9700a9466a5b5cab",
      "c31eabca39b54358bc520a021199c288",
      "84f81b1e03044c52afdfc74c00594036",
      "bdca21ebe0454611ade685d7b4a21375",
      "c5377a24d6574719b9dd47d9539cc20f",
      "a6f60956ccec4d76a6061612e0c6d33d",
      "2b4d6cec2e404793b26a9c251182c75b",
      "632bfdd22b6345e3ba588d40e3c5cfba",
      "ba55ca6ee882407c804b735c67c29ff3",
      "5b384be15bf744c0901c1c087dee79e9",
      "24c0d40ff64d418bb0c36a5b7e94e6e8",
      "51f858a919394c46a029a2297ef86466",
      "d006e22b21a04f7797cf0abc49e18f45",
      "29f3bc2dda7c4b8e98f16346aa8ae146",
      "34bfb14aeea7463f9086b61e800eacdd",
      "a7dc40ee359441b7af087c89765f319c",
      "40aafdbf178744668fe3ad972833bfe4",
      "d742656128c34230b7d79211b8d870f7",
      "62e104103db74e3fad4b48f793703ab6",
      "c92ec273018f486ebd5538cc7b82d6cb",
      "5503addf574b47d1a2db37516258185d",
      "57e3b30799c6490aa4539eb22d50e31f",
      "d5b3e9dc20e64318a6d9088d9594521a",
      "4e25eb0f3f87447baaa7b501feb2eede",
      "3f63a4d2689d44529982026a365c8538",
      "a3d9c89f59ce4e8db4f2e26b4bed70b3",
      "6d0b1924d1514b8b98903c25aaf6e583",
      "3b2420fbc10542a7a199225b0c8eb110",
      "533d3b65f5bc4dcc9ffe672bf53a5c07",
      "5ab2e6b9e77c432093b34b8a16d8fca6",
      "ebdb433548c94c25bfd0626669b535fc",
      "803b6c526cf246a79c9ee6cadf507b5a",
      "36fc005f1382448a8cf1f73b5d4cde25",
      "7344d5ecb2e743d59423f45c01aedf07",
      "02e9dd570a2f40e79199cd2ded39b1c5",
      "f62cc6c803c547a784509acc74467357",
      "58e8155a538e438f9eb896b8f9d0d333",
      "af067d414def45c0acf43fb9d202d794",
      "cbec844a94744a84827b1f149e7ebdc7",
      "ef8bbc5c5f754081bd3bd45a0fef3526",
      "0dd7c41b96514b3090b4054d19e4e447",
      "e110f8bf32174cfa85feff6800789f53",
      "34ab681a2d9246828cfd2c57379f43ca",
      "726b8db145a5441591aa159033d1cfd1",
      "11669d77f1004d7ca68d43b5aa44483e",
      "4f2f930dab304791919c11279ea1a8c5",
      "aac623b35dd34580825f8518c81c44a0",
      "0b3619f2747e43dc9f95c0e898252f72",
      "8ee213624c0f4789981c002436144b09",
      "02aa93fbdc4849f1be1af61523e3f2af",
      "3c1c26da92564bf0a810ada0086e110c",
      "e0ee1654573340b6b5fe146024e807d4",
      "dac3729095dd4cd6832f30644d80c11c",
      "ef73b4a84eb84d07a1c6662a7f6dc26e",
      "869d7bcfb478426ab857d75e07f7c41e",
      "e2f69feed5654bb48d2d8b7d4b30882b",
      "d83490c89ebc4124af2c357b603a5550",
      "ad15ef1b34a14cdda0f12b4a87aa55f6",
      "7888ae74a0eb49efa66fd8780b3adb4f",
      "723888ad118f4954bbe05345f86dbf1f",
      "ea499bc679814695a75ba77eb3966282",
      "33f8d804e2714a58929a3aff6231072a",
      "a78211984bdd439498880ea20a8ca21f",
      "c60c714b0bf443aca4fec8e90ff3accf",
      "23face7c43114ebf883f6b8b7394be60",
      "f083213a468b42c8912eab1d3b951287",
      "1c189fdb6d92402788e20b4aeec074d7",
      "34ace178657b48e48ac4e9b7185dc23b",
      "d6a645a3591b45308556ffb57d9c16c0",
      "f1f133c0dbc3426e9234eeea406fe7df",
      "068fd5f9dcbd48b2ae5b5529c5880f3a",
      "2e1ec9cc529d416e871f3bade5b240c3",
      "3841de8a13ee436a9ff9d4cf77d2d013",
      "ffb3145ecc834bd4aa098099d1a81f9a",
      "eee70011951147b3940116b7774fa56b",
      "55cc55a50b76431199633b29232ed5c9",
      "2cf29efdbd9b4abda26cbd5fd7c42ea3",
      "994d74d53c584954a8d8a75a3a12cd81",
      "2e7f11348c5a4551905079e071b0b9d0",
      "52b918bb4f114d94b07c62a32f63d10c",
      "fe077bb94940462f8de2772ff8a9f6cb",
      "2f9a678c28ce43b0989553021a01f97b",
      "9bab6dbbcba842c8b85315ef617cfbf7",
      "925d98df9a754e03a6de00ace17ae155",
      "4fa45ae91b7a403db000814b4855ba15",
      "bd8f6c55bcb64760a7621fead095f9f0",
      "89abcbcaf4ef4ba683533687486f0544",
      "48affc8059fb442e8e2ef7b2ea78a4dd",
      "9570888b85fd4521839912de9aa08988",
      "8730c9ca4cf34754ae780b670a1c14da",
      "85513f5e10f74ddcbb5fef42721ea9ec",
      "6321215361d8429a9201e5295d815a43",
      "a334afb733ec4b47958ade1b5dcba497",
      "b920c6f20d52486d8fef9e0aa059bdf3",
      "2c1fcc0b433143cebb306f3edea0e670",
      "13463498e3bc40c49c150cd23d8346bc",
      "9bc145eccc0942d4961c306e4e41ba52",
      "4f64f64a55b84719b0a7133cca5769f1",
      "0a542040034a4717be34f075436535c5",
      "5b163841fb374ca2a0a3a09a260358f5",
      "c2388dd52dbd4c0f868ae3c7c89571c6",
      "f2c517afb5b545ddacc532f0086125dc",
      "fb2a983fbf974db19d1baaac724e1bbb",
      "3e7ca99f19a24165b4fc54fb6aba9751",
      "89cdc74fcb004f4caf1c262504dd7a42"
     ]
    },
    "id": "2dd2f802",
    "outputId": "bb13dede-ee19-4628-e3be-24223ee9bf39"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize\n",
    "    processor = DocumentProcessor(Config)\n",
    "\n",
    "    # Check if we already did OCR to save time/cost\n",
    "    if not os.path.exists(Config.OUTPUT_JSON):\n",
    "        raw_data = processor.process_pdf()\n",
    "    else:\n",
    "        print(\"üìÇ Found existing OCR data. Loading...\")\n",
    "        with open(Config.OUTPUT_JSON, \"r\") as f:\n",
    "            raw_data = json.load(f)\n",
    "\n",
    "    # Build Knowledge Base\n",
    "    kb = KnowledgeBase(Config)\n",
    "    kb.chunk_data(raw_data)\n",
    "    kb.build_index()\n",
    "\n",
    "    # Start Chat\n",
    "    bot = RAGAssistant(Config, kb)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ü§ñ GITA AI ASSISTANT READY\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Example Interaction Loop\n",
    "    while True:\n",
    "        q = input(\"\\nAsk a question (or 'exit'): \")\n",
    "        if q.lower() in ['exit', 'quit']: break\n",
    "\n",
    "        print(\"Thinking...\")\n",
    "        answer = bot.ask(q)\n",
    "        print(f\"\\nAnswer:\\n{answer}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
